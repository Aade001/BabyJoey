{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo \n",
    "\n",
    "Most of work is done and only the following needs to be done\n",
    "\n",
    "AutoUnit-**trainging step**  https://pytorch.org/tnt/stable/framework/auto_unit.html\n",
    "- add login W&B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remote/u1138167/BabyJoey/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.optim as optim\n",
    "from torchtnt.framework.auto_unit import AutoUnit\n",
    "from torchtnt.framework.callback import Callback\n",
    "from torchtnt.framework.fit import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to save/load the datasets\n",
    "train_file = 'training_dataset.pt'\n",
    "valid_file = 'validation_dataset.pt'\n",
    "\n",
    "# Data and model configuration\n",
    "data = \"SouthernCrossAI/Project_Gutenberg_Australia\"\n",
    "sequence_length = 512\n",
    "batch_size = 32\n",
    "vocab_size = 50257\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer_decoder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(\n",
    "        dataset['Paragraph'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing transformed datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1030273/334519769.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(train_file)\n",
      "/tmp/ipykernel_1030273/334519769.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  validation_dataset = torch.load(valid_file)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and tokenize datasets\n",
    "if os.path.exists(train_file) and os.path.exists(valid_file):\n",
    "    # Load the transformed datasets\n",
    "    training_dataset = torch.load(train_file)\n",
    "    validation_dataset = torch.load(valid_file)\n",
    "    print(\"Loaded existing transformed datasets.\")\n",
    "else:\n",
    "    # Load the raw dataset\n",
    "    ds = load_dataset(data)\n",
    "    dataset = ds['train'].train_test_split(test_size=0.2)\n",
    "    \n",
    "    # Tokenize and transform the datasets\n",
    "    training_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    validation_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    training_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    # Save for future use\n",
    "    torch.save(training_dataset, train_file)\n",
    "    torch.save(validation_dataset, valid_file)\n",
    "    print(\"Transformed datasets created and saved.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training and validation datasets\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n",
    "        positions = self.position_embedding(positions)\n",
    "        return tokens + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # Transpose for MultiheadAttention (seq_len, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Generate causal mask (triangular lower matrix to prevent attending to future tokens)\n",
    "        seq_len = x.size(0)  # Sequence length after transpose\n",
    "        attn_mask = torch.tril(torch.ones((seq_len, seq_len), device=x.device)).bool()  # Convert to boolean\n",
    "\n",
    "        # Expand the causal mask to match the batch size and attention heads\n",
    "        attn_mask = attn_mask.unsqueeze(0).expand(x.size(1) * n_head, -1, -1)\n",
    "\n",
    "        # Pass the mask to the attention layer\n",
    "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        x = x + attn_output\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Transpose back to (batch_size, seq_len, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + mlp_output\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyJoey(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyJoey, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings()\n",
    "        \n",
    "        # Decoder Blocks (based on n_layer_decoder)\n",
    "        self.decoder_blocks = nn.ModuleList([TransformerBlock() for _ in range(n_layer_decoder)])\n",
    "\n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # Get embeddings\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Apply decoder blocks with attention mask\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, key_padding_mask=key_padding_mask)  # Pass key_padding_mask to the block\n",
    "\n",
    "        # Layer norm and output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyJoeyUnit(AutoUnit):\n",
    "    def __init__(self, module, device=None):\n",
    "        super().__init__(module=module, device=device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, state, data):\n",
    "        input_ids, attention_mask = data['input_ids'], data['attention_mask']\n",
    "        \n",
    "        key_padding_mask = (attention_mask == 0).bool()\n",
    "        logits = self.module(input_ids, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        # Shift input ids by one to get the target sequence\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "    def configure_optimizers_and_lr_scheduler(self, module):\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(module.parameters(), lr=1e-5, weight_decay=1e-3)  # Smaller learning rate and weight decay\n",
    "\n",
    "        # Implement a learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "        \n",
    "        return optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for training\n",
    "class TestingCallback(Callback):\n",
    "    def on_train_start(self, state, unit):\n",
    "        print(\"Training started!\")\n",
    "    def on_train_end(self, state, unit):\n",
    "        print(\"Training ended!\")\n",
    "    def on_train_epoch_start(self, state, unit):\n",
    "        print(f\"Training epoch {unit.train_progress.num_epochs_completed} started!\")\n",
    "    def on_train_epoch_end(self, state, unit):\n",
    "        print(f\"Training epoch {unit.train_progress.num_epochs_completed} ended!\")\n",
    "    def on_train_step_start(self, state, unit):\n",
    "        print(f\"Training batch {unit.train_progress.num_steps_completed} started!\")\n",
    "    def on_train_step_end(self, state, unit):\n",
    "        print(f\"Training batch {unit.train_progress.num_steps_completed} ended!\")\n",
    "    def on_eval_start(self, state, unit):\n",
    "        print(\"Evaluation started!\")\n",
    "    def on_eval_end(self, state, unit):\n",
    "        print(\"Evaluation ended!\")\n",
    "    def on_eval_epoch_start(self, state, unit):\n",
    "        print(f\"Evaluation epoch {unit.eval_progress.num_epochs_completed} started!\")\n",
    "    def on_eval_epoch_end(self, state, unit):\n",
    "        print(f\"Evaluation epoch {unit.eval_progress.num_epochs_completed} ended!\")\n",
    "    def on_eval_step_start(self, state, unit):\n",
    "        print(f\"Evaluation batch {unit.eval_progress.num_steps_completed} started!\")\n",
    "    def on_eval_step_end(self, state, unit):\n",
    "        print(f\"Evaluation batch {unit.eval_progress.num_steps_completed} ended!\")\n",
    "    def on_exception(self, state, unit, exc: BaseException):\n",
    "        print(f\"Exception occurred: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device and initialize the BabyJoey model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BabyJoey().to(device)\n",
    "baby_joey_unit = BabyJoeyUnit(module=model, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in training_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n",
      "Training epoch 0 started!\n",
      "Training batch 0 started!\n",
      "Training batch 1 ended!\n",
      "Training batch 1 started!\n",
      "Training batch 2 ended!\n",
      "Training batch 2 started!\n",
      "Training batch 3 ended!\n",
      "Training batch 3 started!\n",
      "Training batch 4 ended!\n",
      "Training batch 4 started!\n",
      "Training batch 5 ended!\n",
      "Training batch 5 started!\n",
      "Training batch 6 ended!\n",
      "Training batch 6 started!\n",
      "Training batch 7 ended!\n",
      "Training batch 7 started!\n",
      "Training batch 8 ended!\n",
      "Training batch 8 started!\n",
      "Training batch 9 ended!\n",
      "Training batch 9 started!\n",
      "Training batch 10 ended!\n",
      "Training batch 10 started!\n",
      "Training batch 11 ended!\n",
      "Training batch 11 started!\n",
      "Training batch 12 ended!\n",
      "Training batch 12 started!\n",
      "Training batch 13 ended!\n",
      "Training batch 13 started!\n",
      "Training batch 14 ended!\n",
      "Training batch 14 started!\n",
      "Training batch 15 ended!\n",
      "Training batch 15 started!\n",
      "Training batch 16 ended!\n",
      "Training batch 16 started!\n",
      "Training batch 17 ended!\n",
      "Training batch 17 started!\n",
      "Training batch 18 ended!\n",
      "Training batch 18 started!\n",
      "Training batch 19 ended!\n",
      "Training batch 19 started!\n",
      "Training batch 20 ended!\n",
      "Training batch 20 started!\n",
      "Training batch 21 ended!\n",
      "Training batch 21 started!\n",
      "Training batch 22 ended!\n",
      "Training batch 22 started!\n",
      "Training batch 23 ended!\n",
      "Training batch 23 started!\n",
      "Training batch 24 ended!\n",
      "Training batch 24 started!\n",
      "Training batch 25 ended!\n",
      "Training batch 25 started!\n",
      "Training batch 26 ended!\n",
      "Training batch 26 started!\n",
      "Training batch 27 ended!\n",
      "Training batch 27 started!\n",
      "Training batch 28 ended!\n",
      "Training batch 28 started!\n",
      "Training batch 29 ended!\n",
      "Training batch 29 started!\n",
      "Training batch 30 ended!\n",
      "Training batch 30 started!\n",
      "Training batch 31 ended!\n",
      "Training batch 31 started!\n",
      "Training batch 32 ended!\n",
      "Training batch 32 started!\n",
      "Training batch 33 ended!\n",
      "Training batch 33 started!\n",
      "Training batch 34 ended!\n",
      "Training batch 34 started!\n",
      "Training batch 35 ended!\n",
      "Training batch 35 started!\n",
      "Training batch 36 ended!\n",
      "Training batch 36 started!\n",
      "Training batch 37 ended!\n",
      "Training batch 37 started!\n",
      "Training batch 38 ended!\n",
      "Training batch 38 started!\n",
      "Training batch 39 ended!\n",
      "Training batch 39 started!\n",
      "Training batch 40 ended!\n",
      "Training batch 40 started!\n",
      "Training batch 41 ended!\n",
      "Training batch 41 started!\n",
      "Training batch 42 ended!\n",
      "Training batch 42 started!\n",
      "Training batch 43 ended!\n",
      "Training batch 43 started!\n",
      "Training batch 44 ended!\n",
      "Training batch 44 started!\n",
      "Training batch 45 ended!\n",
      "Training batch 45 started!\n",
      "Training batch 46 ended!\n",
      "Training batch 46 started!\n",
      "Training batch 47 ended!\n",
      "Training batch 47 started!\n",
      "Training batch 48 ended!\n",
      "Training batch 48 started!\n",
      "Training batch 49 ended!\n",
      "Training batch 49 started!\n",
      "Training batch 50 ended!\n",
      "Training batch 50 started!\n",
      "Training batch 51 ended!\n",
      "Training batch 51 started!\n",
      "Training batch 52 ended!\n",
      "Training batch 52 started!\n",
      "Training batch 53 ended!\n",
      "Training batch 53 started!\n",
      "Training batch 54 ended!\n",
      "Training batch 54 started!\n",
      "Training batch 55 ended!\n",
      "Training batch 55 started!\n",
      "Training batch 56 ended!\n",
      "Training batch 56 started!\n",
      "Training batch 57 ended!\n",
      "Training batch 57 started!\n",
      "Training batch 58 ended!\n",
      "Training batch 58 started!\n",
      "Training batch 59 ended!\n",
      "Training batch 59 started!\n",
      "Training batch 60 ended!\n",
      "Training batch 60 started!\n",
      "Training batch 61 ended!\n",
      "Training batch 61 started!\n",
      "Training batch 62 ended!\n",
      "Training batch 62 started!\n",
      "Training batch 63 ended!\n",
      "Training batch 63 started!\n",
      "Training batch 64 ended!\n",
      "Training batch 64 started!\n",
      "Training batch 65 ended!\n",
      "Training batch 65 started!\n",
      "Training batch 66 ended!\n",
      "Training batch 66 started!\n",
      "Training batch 67 ended!\n",
      "Training batch 67 started!\n",
      "Training batch 68 ended!\n",
      "Training batch 68 started!\n",
      "Training batch 69 ended!\n",
      "Training batch 69 started!\n",
      "Training batch 70 ended!\n",
      "Training batch 70 started!\n",
      "Training batch 71 ended!\n",
      "Training batch 71 started!\n",
      "Training batch 72 ended!\n",
      "Training batch 72 started!\n",
      "Training batch 73 ended!\n",
      "Training batch 73 started!\n",
      "Training batch 74 ended!\n",
      "Training batch 74 started!\n",
      "Training batch 75 ended!\n",
      "Training batch 75 started!\n",
      "Training batch 76 ended!\n",
      "Training batch 76 started!\n",
      "Training batch 77 ended!\n",
      "Training batch 77 started!\n",
      "Training batch 78 ended!\n",
      "Training batch 78 started!\n",
      "Training batch 79 ended!\n",
      "Training batch 79 started!\n",
      "Training batch 80 ended!\n",
      "Training batch 80 started!\n",
      "Training batch 81 ended!\n",
      "Training batch 81 started!\n",
      "Training batch 82 ended!\n",
      "Training batch 82 started!\n",
      "Training batch 83 ended!\n",
      "Training batch 83 started!\n",
      "Training batch 84 ended!\n",
      "Training batch 84 started!\n",
      "Training batch 85 ended!\n",
      "Training batch 85 started!\n",
      "Training batch 86 ended!\n",
      "Training batch 86 started!\n",
      "Training batch 87 ended!\n",
      "Training batch 87 started!\n",
      "Training batch 88 ended!\n",
      "Training batch 88 started!\n",
      "Training batch 89 ended!\n",
      "Training batch 89 started!\n",
      "Training batch 90 ended!\n",
      "Training batch 90 started!\n",
      "Training batch 91 ended!\n",
      "Training batch 91 started!\n",
      "Training batch 92 ended!\n",
      "Training batch 92 started!\n",
      "Training batch 93 ended!\n",
      "Training batch 93 started!\n",
      "Training batch 94 ended!\n",
      "Training batch 94 started!\n",
      "Training batch 95 ended!\n",
      "Training batch 95 started!\n",
      "Training batch 96 ended!\n",
      "Training batch 96 started!\n",
      "Training batch 97 ended!\n",
      "Training batch 97 started!\n",
      "Training batch 98 ended!\n",
      "Training batch 98 started!\n",
      "Training batch 99 ended!\n",
      "Training batch 99 started!\n",
      "Training batch 100 ended!\n",
      "Training batch 100 started!\n",
      "Training batch 101 ended!\n",
      "Training batch 101 started!\n",
      "Training batch 102 ended!\n",
      "Training batch 102 started!\n",
      "Training batch 103 ended!\n",
      "Training batch 103 started!\n",
      "Training batch 104 ended!\n",
      "Training batch 104 started!\n",
      "Training batch 105 ended!\n",
      "Training batch 105 started!\n",
      "Training batch 106 ended!\n",
      "Training batch 106 started!\n",
      "Training batch 107 ended!\n",
      "Training batch 107 started!\n",
      "Training batch 108 ended!\n",
      "Training batch 108 started!\n",
      "Training batch 109 ended!\n",
      "Training batch 109 started!\n",
      "Training batch 110 ended!\n",
      "Training batch 110 started!\n",
      "Training batch 111 ended!\n",
      "Training batch 111 started!\n",
      "Training batch 112 ended!\n",
      "Training batch 112 started!\n",
      "Training batch 113 ended!\n",
      "Training batch 113 started!\n",
      "Training batch 114 ended!\n",
      "Training batch 114 started!\n",
      "Training batch 115 ended!\n",
      "Training batch 115 started!\n",
      "Training batch 116 ended!\n",
      "Training batch 116 started!\n",
      "Training batch 117 ended!\n",
      "Training batch 117 started!\n",
      "Training batch 118 ended!\n",
      "Training batch 118 started!\n",
      "Training batch 119 ended!\n",
      "Training batch 119 started!\n",
      "Training batch 120 ended!\n",
      "Training batch 120 started!\n",
      "Training batch 121 ended!\n",
      "Training batch 121 started!\n",
      "Training batch 122 ended!\n",
      "Training batch 122 started!\n",
      "Training batch 123 ended!\n",
      "Training batch 123 started!\n",
      "Training batch 124 ended!\n",
      "Training batch 124 started!\n",
      "Training batch 125 ended!\n",
      "Training batch 125 started!\n",
      "Training batch 126 ended!\n",
      "Training batch 126 started!\n",
      "Training batch 127 ended!\n",
      "Training batch 127 started!\n",
      "Training batch 128 ended!\n",
      "Training batch 128 started!\n",
      "Training batch 129 ended!\n",
      "Training batch 129 started!\n",
      "Training batch 130 ended!\n",
      "Training batch 130 started!\n",
      "Training batch 131 ended!\n",
      "Training batch 131 started!\n",
      "Training batch 132 ended!\n",
      "Training batch 132 started!\n",
      "Training batch 133 ended!\n",
      "Training batch 133 started!\n",
      "Training batch 134 ended!\n",
      "Training batch 134 started!\n",
      "Training batch 135 ended!\n",
      "Training batch 135 started!\n",
      "Training batch 136 ended!\n",
      "Training batch 136 started!\n",
      "Training batch 137 ended!\n",
      "Training batch 137 started!\n",
      "Training batch 138 ended!\n",
      "Training batch 138 started!\n",
      "Training batch 139 ended!\n",
      "Training batch 139 started!\n",
      "Training batch 140 ended!\n",
      "Training batch 140 started!\n",
      "Training batch 141 ended!\n",
      "Training batch 141 started!\n",
      "Training batch 142 ended!\n",
      "Training batch 142 started!\n",
      "Training batch 143 ended!\n",
      "Training batch 143 started!\n",
      "Training batch 144 ended!\n",
      "Training batch 144 started!\n",
      "Training batch 145 ended!\n",
      "Training batch 145 started!\n",
      "Training batch 146 ended!\n",
      "Training batch 146 started!\n",
      "Training batch 147 ended!\n",
      "Training batch 147 started!\n",
      "Training batch 148 ended!\n",
      "Training batch 148 started!\n",
      "Training batch 149 ended!\n",
      "Training batch 149 started!\n",
      "Training batch 150 ended!\n",
      "Training batch 150 started!\n",
      "Training batch 151 ended!\n",
      "Training batch 151 started!\n",
      "Training batch 152 ended!\n",
      "Training batch 152 started!\n",
      "Training batch 153 ended!\n",
      "Training batch 153 started!\n",
      "Training batch 154 ended!\n",
      "Training batch 154 started!\n",
      "Training batch 155 ended!\n",
      "Training batch 155 started!\n",
      "Training batch 156 ended!\n",
      "Training batch 156 started!\n",
      "Training batch 157 ended!\n",
      "Training batch 157 started!\n",
      "Training batch 158 ended!\n",
      "Training batch 158 started!\n",
      "Training batch 159 ended!\n",
      "Training batch 159 started!\n",
      "Training batch 160 ended!\n",
      "Training batch 160 started!\n",
      "Training batch 161 ended!\n",
      "Training batch 161 started!\n",
      "Training batch 162 ended!\n",
      "Training batch 162 started!\n",
      "Training batch 163 ended!\n",
      "Training batch 163 started!\n",
      "Training batch 164 ended!\n",
      "Training batch 164 started!\n",
      "Training batch 165 ended!\n",
      "Training batch 165 started!\n",
      "Training batch 166 ended!\n",
      "Training batch 166 started!\n",
      "Training batch 167 ended!\n",
      "Training batch 167 started!\n",
      "Training batch 168 ended!\n",
      "Training batch 168 started!\n",
      "Training batch 169 ended!\n",
      "Training batch 169 started!\n",
      "Training batch 170 ended!\n",
      "Training batch 170 started!\n",
      "Training batch 171 ended!\n",
      "Training batch 171 started!\n",
      "Training batch 172 ended!\n",
      "Training batch 172 started!\n",
      "Training batch 173 ended!\n",
      "Training batch 173 started!\n",
      "Training batch 174 ended!\n",
      "Training batch 174 started!\n",
      "Training batch 175 ended!\n",
      "Training batch 175 started!\n",
      "Training batch 176 ended!\n",
      "Training batch 176 started!\n",
      "Training batch 177 ended!\n",
      "Training batch 177 started!\n",
      "Training batch 178 ended!\n",
      "Training batch 178 started!\n",
      "Training batch 179 ended!\n",
      "Training batch 179 started!\n",
      "Training batch 180 ended!\n",
      "Training batch 180 started!\n",
      "Training batch 181 ended!\n",
      "Training batch 181 started!\n",
      "Training batch 182 ended!\n",
      "Training batch 182 started!\n",
      "Training batch 183 ended!\n",
      "Training batch 183 started!\n",
      "Training batch 184 ended!\n",
      "Training batch 184 started!\n",
      "Training batch 185 ended!\n",
      "Training batch 185 started!\n",
      "Training batch 186 ended!\n",
      "Training batch 186 started!\n",
      "Training batch 187 ended!\n",
      "Training batch 187 started!\n",
      "Training batch 188 ended!\n",
      "Training batch 188 started!\n",
      "Training batch 189 ended!\n",
      "Training batch 189 started!\n",
      "Training batch 190 ended!\n",
      "Training batch 190 started!\n",
      "Training batch 191 ended!\n",
      "Training batch 191 started!\n",
      "Training batch 192 ended!\n",
      "Training batch 192 started!\n",
      "Training batch 193 ended!\n",
      "Training batch 193 started!\n",
      "Training batch 194 ended!\n",
      "Training batch 194 started!\n",
      "Training batch 195 ended!\n",
      "Training batch 195 started!\n",
      "Training batch 196 ended!\n",
      "Training batch 196 started!\n",
      "Training batch 197 ended!\n",
      "Training batch 197 started!\n",
      "Training batch 198 ended!\n",
      "Training batch 198 started!\n",
      "Training batch 199 ended!\n",
      "Training batch 199 started!\n",
      "Training batch 200 ended!\n",
      "Training batch 200 started!\n",
      "Training batch 201 ended!\n",
      "Training batch 201 started!\n",
      "Training batch 202 ended!\n",
      "Training batch 202 started!\n",
      "Training batch 203 ended!\n",
      "Training batch 203 started!\n",
      "Training batch 204 ended!\n",
      "Training batch 204 started!\n",
      "Training batch 205 ended!\n",
      "Training batch 205 started!\n",
      "Training batch 206 ended!\n",
      "Training batch 206 started!\n",
      "Training batch 207 ended!\n",
      "Training batch 207 started!\n",
      "Training batch 208 ended!\n",
      "Training batch 208 started!\n",
      "Training batch 209 ended!\n",
      "Training batch 209 started!\n",
      "Training batch 210 ended!\n",
      "Training batch 210 started!\n",
      "Training batch 211 ended!\n",
      "Training batch 211 started!\n",
      "Training batch 212 ended!\n",
      "Training batch 212 started!\n",
      "Training batch 213 ended!\n",
      "Training batch 213 started!\n",
      "Training batch 214 ended!\n",
      "Training batch 214 started!\n",
      "Training batch 215 ended!\n",
      "Training batch 215 started!\n",
      "Training batch 216 ended!\n",
      "Training batch 216 started!\n",
      "Training batch 217 ended!\n",
      "Training batch 217 started!\n",
      "Training batch 218 ended!\n",
      "Training batch 218 started!\n",
      "Training batch 219 ended!\n",
      "Training batch 219 started!\n",
      "Training batch 220 ended!\n",
      "Training batch 220 started!\n",
      "Training batch 221 ended!\n",
      "Training batch 221 started!\n",
      "Training batch 222 ended!\n",
      "Training batch 222 started!\n",
      "Training batch 223 ended!\n",
      "Training batch 223 started!\n",
      "Training batch 224 ended!\n",
      "Training batch 224 started!\n",
      "Training batch 225 ended!\n",
      "Training batch 225 started!\n",
      "Training batch 226 ended!\n",
      "Training batch 226 started!\n",
      "Training batch 227 ended!\n",
      "Training batch 227 started!\n",
      "Training batch 228 ended!\n",
      "Training batch 228 started!\n",
      "Training batch 229 ended!\n",
      "Training batch 229 started!\n",
      "Training batch 230 ended!\n",
      "Training batch 230 started!\n",
      "Training batch 231 ended!\n",
      "Training batch 231 started!\n",
      "Training batch 232 ended!\n",
      "Training batch 232 started!\n",
      "Training batch 233 ended!\n",
      "Training batch 233 started!\n",
      "Training batch 234 ended!\n",
      "Training batch 234 started!\n",
      "Training batch 235 ended!\n",
      "Training batch 235 started!\n",
      "Training batch 236 ended!\n",
      "Training batch 236 started!\n",
      "Training batch 237 ended!\n",
      "Training batch 237 started!\n",
      "Training batch 238 ended!\n",
      "Training batch 238 started!\n",
      "Training batch 239 ended!\n",
      "Training batch 239 started!\n",
      "Training batch 240 ended!\n",
      "Training batch 240 started!\n",
      "Training batch 241 ended!\n",
      "Training batch 241 started!\n",
      "Training batch 242 ended!\n",
      "Training batch 242 started!\n",
      "Training batch 243 ended!\n",
      "Training batch 243 started!\n",
      "Training batch 244 ended!\n",
      "Training batch 244 started!\n",
      "Training batch 245 ended!\n",
      "Training batch 245 started!\n",
      "Training batch 246 ended!\n",
      "Training batch 246 started!\n",
      "Training batch 247 ended!\n",
      "Training batch 247 started!\n",
      "Training batch 248 ended!\n",
      "Training batch 248 started!\n",
      "Training batch 249 ended!\n",
      "Training batch 249 started!\n",
      "Training batch 250 ended!\n",
      "Training batch 250 started!\n",
      "Training batch 251 ended!\n",
      "Training batch 251 started!\n",
      "Training batch 252 ended!\n",
      "Training batch 252 started!\n",
      "Training batch 253 ended!\n",
      "Training batch 253 started!\n",
      "Training batch 254 ended!\n",
      "Training batch 254 started!\n",
      "Training batch 255 ended!\n",
      "Training batch 255 started!\n",
      "Training batch 256 ended!\n",
      "Training batch 256 started!\n",
      "Training batch 257 ended!\n",
      "Training batch 257 started!\n",
      "Training batch 258 ended!\n",
      "Training batch 258 started!\n",
      "Training batch 259 ended!\n",
      "Training batch 259 started!\n",
      "Training batch 260 ended!\n",
      "Training batch 260 started!\n",
      "Training batch 261 ended!\n",
      "Training batch 261 started!\n",
      "Training batch 262 ended!\n",
      "Training batch 262 started!\n",
      "Training batch 263 ended!\n",
      "Training batch 263 started!\n",
      "Training batch 264 ended!\n",
      "Training batch 264 started!\n",
      "Training batch 265 ended!\n",
      "Training batch 265 started!\n",
      "Training batch 266 ended!\n",
      "Training batch 266 started!\n",
      "Training batch 267 ended!\n",
      "Training batch 267 started!\n",
      "Training batch 268 ended!\n",
      "Training batch 268 started!\n",
      "Training batch 269 ended!\n",
      "Training batch 269 started!\n",
      "Training batch 270 ended!\n",
      "Training batch 270 started!\n",
      "Training batch 271 ended!\n",
      "Training batch 271 started!\n",
      "Training batch 272 ended!\n",
      "Training batch 272 started!\n",
      "Training batch 273 ended!\n",
      "Training batch 273 started!\n",
      "Training batch 274 ended!\n",
      "Training batch 274 started!\n",
      "Training batch 275 ended!\n",
      "Training batch 275 started!\n",
      "Training batch 276 ended!\n",
      "Training batch 276 started!\n",
      "Training batch 277 ended!\n",
      "Training batch 277 started!\n",
      "Training batch 278 ended!\n",
      "Training batch 278 started!\n",
      "Training batch 279 ended!\n",
      "Training batch 279 started!\n",
      "Training batch 280 ended!\n",
      "Training batch 280 started!\n",
      "Training batch 281 ended!\n",
      "Training batch 281 started!\n",
      "Training batch 282 ended!\n",
      "Training batch 282 started!\n",
      "Training batch 283 ended!\n",
      "Training batch 283 started!\n",
      "Training batch 284 ended!\n",
      "Training batch 284 started!\n",
      "Training batch 285 ended!\n",
      "Training batch 285 started!\n",
      "Training batch 286 ended!\n",
      "Training batch 286 started!\n",
      "Training batch 287 ended!\n",
      "Training batch 287 started!\n",
      "Training batch 288 ended!\n",
      "Training batch 288 started!\n",
      "Training batch 289 ended!\n",
      "Training batch 289 started!\n",
      "Training batch 290 ended!\n",
      "Training batch 290 started!\n",
      "Training batch 291 ended!\n",
      "Training batch 291 started!\n",
      "Training batch 292 ended!\n",
      "Training batch 292 started!\n",
      "Training batch 293 ended!\n",
      "Training batch 293 started!\n",
      "Training batch 294 ended!\n",
      "Training batch 294 started!\n",
      "Training batch 295 ended!\n",
      "Training batch 295 started!\n",
      "Training batch 296 ended!\n",
      "Training batch 296 started!\n",
      "Training batch 297 ended!\n",
      "Training batch 297 started!\n",
      "Training batch 298 ended!\n",
      "Training batch 298 started!\n",
      "Training batch 299 ended!\n",
      "Training batch 299 started!\n",
      "Training batch 300 ended!\n",
      "Training batch 300 started!\n",
      "Training batch 301 ended!\n",
      "Training batch 301 started!\n",
      "Training batch 302 ended!\n",
      "Training batch 302 started!\n",
      "Training batch 303 ended!\n",
      "Training batch 303 started!\n",
      "Training batch 304 ended!\n",
      "Training batch 304 started!\n",
      "Training batch 305 ended!\n",
      "Training batch 305 started!\n",
      "Training batch 306 ended!\n",
      "Training batch 306 started!\n",
      "Training batch 307 ended!\n",
      "Training batch 307 started!\n",
      "Training batch 308 ended!\n",
      "Training batch 308 started!\n",
      "Training batch 309 ended!\n",
      "Training batch 309 started!\n",
      "Training batch 310 ended!\n",
      "Training batch 310 started!\n",
      "Training batch 311 ended!\n",
      "Training batch 311 started!\n",
      "Training batch 312 ended!\n",
      "Training batch 312 started!\n",
      "Training batch 313 ended!\n",
      "Training batch 313 started!\n",
      "Training batch 314 ended!\n",
      "Training batch 314 started!\n",
      "Training batch 315 ended!\n",
      "Training batch 315 started!\n",
      "Training batch 316 ended!\n",
      "Training batch 316 started!\n",
      "Training batch 317 ended!\n",
      "Training batch 317 started!\n",
      "Training batch 318 ended!\n",
      "Training batch 318 started!\n",
      "Training batch 319 ended!\n",
      "Training batch 319 started!\n",
      "Training batch 320 ended!\n",
      "Training batch 320 started!\n",
      "Training batch 321 ended!\n",
      "Training batch 321 started!\n",
      "Training batch 322 ended!\n",
      "Training batch 322 started!\n",
      "Training batch 323 ended!\n",
      "Training batch 323 started!\n",
      "Training batch 324 ended!\n",
      "Training batch 324 started!\n",
      "Training batch 325 ended!\n",
      "Training batch 325 started!\n",
      "Training batch 326 ended!\n",
      "Training batch 326 started!\n",
      "Training batch 327 ended!\n",
      "Training batch 327 started!\n",
      "Training batch 328 ended!\n",
      "Training batch 328 started!\n",
      "Training batch 329 ended!\n",
      "Training batch 329 started!\n",
      "Training batch 330 ended!\n",
      "Training batch 330 started!\n",
      "Training batch 331 ended!\n",
      "Training batch 331 started!\n",
      "Training batch 332 ended!\n",
      "Training batch 332 started!\n",
      "Training batch 333 ended!\n",
      "Training batch 333 started!\n",
      "Training batch 334 ended!\n",
      "Training batch 334 started!\n",
      "Training batch 335 ended!\n",
      "Training batch 335 started!\n",
      "Training batch 336 ended!\n",
      "Training batch 336 started!\n",
      "Training batch 337 ended!\n",
      "Training batch 337 started!\n",
      "Training batch 338 ended!\n",
      "Training batch 338 started!\n",
      "Training batch 339 ended!\n",
      "Training batch 339 started!\n",
      "Training batch 340 ended!\n",
      "Training batch 340 started!\n",
      "Training batch 341 ended!\n",
      "Training batch 341 started!\n",
      "Training batch 342 ended!\n",
      "Training batch 342 started!\n",
      "Training batch 343 ended!\n",
      "Training batch 343 started!\n",
      "Training batch 344 ended!\n",
      "Training batch 344 started!\n",
      "Training batch 345 ended!\n",
      "Training batch 345 started!\n",
      "Training batch 346 ended!\n",
      "Training batch 346 started!\n",
      "Training batch 347 ended!\n",
      "Training batch 347 started!\n",
      "Training batch 348 ended!\n",
      "Training batch 348 started!\n",
      "Training batch 349 ended!\n",
      "Training batch 349 started!\n",
      "Training batch 350 ended!\n",
      "Training batch 350 started!\n",
      "Training batch 351 ended!\n",
      "Training batch 351 started!\n",
      "Training batch 352 ended!\n",
      "Training batch 352 started!\n",
      "Training batch 353 ended!\n",
      "Training batch 353 started!\n",
      "Training batch 354 ended!\n",
      "Training batch 354 started!\n",
      "Training batch 355 ended!\n",
      "Training batch 355 started!\n",
      "Training batch 356 ended!\n",
      "Training batch 356 started!\n",
      "Training batch 357 ended!\n",
      "Training batch 357 started!\n",
      "Training batch 358 ended!\n",
      "Training batch 358 started!\n",
      "Training batch 359 ended!\n",
      "Training batch 359 started!\n",
      "Training batch 360 ended!\n",
      "Training batch 360 started!\n",
      "Training batch 361 ended!\n",
      "Training batch 361 started!\n",
      "Training batch 362 ended!\n",
      "Training batch 362 started!\n",
      "Training batch 363 ended!\n",
      "Training batch 363 started!\n",
      "Training batch 364 ended!\n",
      "Training batch 364 started!\n",
      "Training batch 365 ended!\n",
      "Training batch 365 started!\n",
      "Training batch 366 ended!\n",
      "Training batch 366 started!\n",
      "Training batch 367 ended!\n",
      "Training batch 367 started!\n",
      "Training batch 368 ended!\n",
      "Training batch 368 started!\n",
      "Training batch 369 ended!\n",
      "Training batch 369 started!\n",
      "Training batch 370 ended!\n",
      "Training batch 370 started!\n",
      "Training batch 371 ended!\n",
      "Training batch 371 started!\n",
      "Training batch 372 ended!\n",
      "Training batch 372 started!\n",
      "Training batch 373 ended!\n",
      "Training batch 373 started!\n",
      "Training batch 374 ended!\n",
      "Training batch 374 started!\n",
      "Training batch 375 ended!\n",
      "Training batch 375 started!\n",
      "Training batch 376 ended!\n",
      "Training batch 376 started!\n",
      "Training batch 377 ended!\n",
      "Training batch 377 started!\n",
      "Training batch 378 ended!\n",
      "Training batch 378 started!\n",
      "Training batch 379 ended!\n",
      "Training batch 379 started!\n",
      "Training batch 380 ended!\n",
      "Training batch 380 started!\n",
      "Training batch 381 ended!\n",
      "Training batch 381 started!\n",
      "Training batch 382 ended!\n",
      "Training batch 382 started!\n",
      "Training batch 383 ended!\n",
      "Training batch 383 started!\n",
      "Training batch 384 ended!\n",
      "Training batch 384 started!\n",
      "Training batch 385 ended!\n",
      "Training batch 385 started!\n",
      "Training batch 386 ended!\n",
      "Training batch 386 started!\n",
      "Training batch 387 ended!\n",
      "Training batch 387 started!\n",
      "Training batch 388 ended!\n",
      "Training batch 388 started!\n",
      "Training batch 389 ended!\n",
      "Training batch 389 started!\n",
      "Training batch 390 ended!\n",
      "Training batch 390 started!\n",
      "Training batch 391 ended!\n",
      "Training batch 391 started!\n",
      "Training batch 392 ended!\n",
      "Training batch 392 started!\n",
      "Training batch 393 ended!\n",
      "Training batch 393 started!\n",
      "Training batch 394 ended!\n",
      "Training batch 394 started!\n",
      "Training batch 395 ended!\n",
      "Training batch 395 started!\n",
      "Training batch 396 ended!\n",
      "Training batch 396 started!\n",
      "Training batch 397 ended!\n",
      "Training batch 397 started!\n",
      "Training batch 398 ended!\n",
      "Training batch 398 started!\n",
      "Training batch 399 ended!\n",
      "Training batch 399 started!\n",
      "Training batch 400 ended!\n",
      "Training batch 400 started!\n",
      "Training batch 401 ended!\n",
      "Training batch 401 started!\n",
      "Training batch 402 ended!\n",
      "Training batch 402 started!\n",
      "Training batch 403 ended!\n",
      "Training batch 403 started!\n",
      "Training batch 404 ended!\n",
      "Training batch 404 started!\n",
      "Training batch 405 ended!\n",
      "Training batch 405 started!\n",
      "Training batch 406 ended!\n",
      "Training batch 406 started!\n",
      "Training batch 407 ended!\n",
      "Training batch 407 started!\n",
      "Training batch 408 ended!\n",
      "Training batch 408 started!\n",
      "Training batch 409 ended!\n",
      "Training batch 409 started!\n",
      "Training batch 410 ended!\n",
      "Training batch 410 started!\n",
      "Training batch 411 ended!\n",
      "Training batch 411 started!\n",
      "Training batch 412 ended!\n",
      "Training batch 412 started!\n",
      "Training batch 413 ended!\n",
      "Training batch 413 started!\n",
      "Training batch 414 ended!\n",
      "Training batch 414 started!\n",
      "Training batch 415 ended!\n",
      "Training batch 415 started!\n",
      "Training batch 416 ended!\n",
      "Training batch 416 started!\n",
      "Training batch 417 ended!\n",
      "Training batch 417 started!\n",
      "Training batch 418 ended!\n",
      "Training batch 418 started!\n",
      "Training batch 419 ended!\n",
      "Training batch 419 started!\n",
      "Training batch 420 ended!\n",
      "Training batch 420 started!\n",
      "Training batch 421 ended!\n",
      "Training batch 421 started!\n",
      "Training batch 422 ended!\n",
      "Training batch 422 started!\n",
      "Training batch 423 ended!\n",
      "Training batch 423 started!\n",
      "Training batch 424 ended!\n",
      "Training batch 424 started!\n",
      "Training batch 425 ended!\n",
      "Training batch 425 started!\n",
      "Training batch 426 ended!\n",
      "Training batch 426 started!\n",
      "Training batch 427 ended!\n",
      "Training batch 427 started!\n",
      "Training batch 428 ended!\n",
      "Training batch 428 started!\n",
      "Training batch 429 ended!\n",
      "Training batch 429 started!\n",
      "Training batch 430 ended!\n",
      "Training batch 430 started!\n",
      "Training batch 431 ended!\n",
      "Training batch 431 started!\n",
      "Training batch 432 ended!\n",
      "Training batch 432 started!\n",
      "Training batch 433 ended!\n",
      "Training batch 433 started!\n",
      "Training batch 434 ended!\n",
      "Training batch 434 started!\n",
      "Training batch 435 ended!\n",
      "Training batch 435 started!\n",
      "Training batch 436 ended!\n",
      "Training batch 436 started!\n",
      "Training batch 437 ended!\n",
      "Training batch 437 started!\n",
      "Training batch 438 ended!\n",
      "Training batch 438 started!\n",
      "Training batch 439 ended!\n",
      "Training batch 439 started!\n",
      "Training batch 440 ended!\n",
      "Training batch 440 started!\n",
      "Training batch 441 ended!\n",
      "Training batch 441 started!\n",
      "Training batch 442 ended!\n",
      "Training batch 442 started!\n",
      "Training batch 443 ended!\n",
      "Training batch 443 started!\n",
      "Training batch 444 ended!\n",
      "Training batch 444 started!\n",
      "Training batch 445 ended!\n",
      "Training batch 445 started!\n",
      "Training batch 446 ended!\n",
      "Training batch 446 started!\n",
      "Training batch 447 ended!\n",
      "Training batch 447 started!\n",
      "Training batch 448 ended!\n",
      "Training batch 448 started!\n",
      "Training batch 449 ended!\n",
      "Training batch 449 started!\n",
      "Training batch 450 ended!\n",
      "Training batch 450 started!\n",
      "Training batch 451 ended!\n",
      "Training batch 451 started!\n",
      "Training batch 452 ended!\n",
      "Training batch 452 started!\n",
      "Training batch 453 ended!\n",
      "Training batch 453 started!\n",
      "Training batch 454 ended!\n",
      "Training batch 454 started!\n",
      "Training batch 455 ended!\n",
      "Training batch 455 started!\n",
      "Training batch 456 ended!\n",
      "Training batch 456 started!\n",
      "Training batch 457 ended!\n",
      "Training batch 457 started!\n",
      "Training batch 458 ended!\n",
      "Training batch 458 started!\n",
      "Training batch 459 ended!\n",
      "Training batch 459 started!\n",
      "Training batch 460 ended!\n",
      "Training batch 460 started!\n",
      "Training batch 461 ended!\n",
      "Training batch 461 started!\n",
      "Training batch 462 ended!\n",
      "Training batch 462 started!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model using the defined AutoUnit and callback\n",
    "fit(\n",
    "    baby_joey_unit,\n",
    "    train_dataloader=training_dataloader,\n",
    "    eval_dataloader=validation_dataloader,\n",
    "    max_epochs=2,\n",
    "    callbacks=[TestingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
