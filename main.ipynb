{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo \n",
    "\n",
    "Most of work is done and only the following needs to be done\n",
    "\n",
    "AutoUnit-**trainging step**  https://pytorch.org/tnt/stable/framework/auto_unit.html\n",
    "- add login W&B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchtnt.framework.auto_unit import AutoUnit\n",
    "from torchtnt.framework.callback import Callback\n",
    "from torchtnt.framework.fit import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to save/load the datasets\n",
    "train_file = 'training_dataset.pt'\n",
    "valid_file = 'validation_dataset.pt'\n",
    "\n",
    "# Data and model configuration\n",
    "data = \"SouthernCrossAI/Project_Gutenberg_Australia\"\n",
    "sequence_length = 512\n",
    "batch_size = 32\n",
    "vocab_size = 50257\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer_decoder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(\n",
    "        dataset['Paragraph'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and tokenize datasets\n",
    "if os.path.exists(train_file) and os.path.exists(valid_file):\n",
    "    # Load the transformed datasets\n",
    "    training_dataset = torch.load(train_file)\n",
    "    validation_dataset = torch.load(valid_file)\n",
    "    print(\"Loaded existing transformed datasets.\")\n",
    "else:\n",
    "    # Load the raw dataset\n",
    "    ds = load_dataset(data)\n",
    "    dataset = ds['train'].train_test_split(test_size=0.2)\n",
    "    \n",
    "    # Tokenize and transform the datasets\n",
    "    training_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    validation_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    training_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    # Save for future use\n",
    "    torch.save(training_dataset, train_file)\n",
    "    torch.save(validation_dataset, valid_file)\n",
    "    print(\"Transformed datasets created and saved.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training and validation datasets\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n",
    "        positions = self.position_embedding(positions)\n",
    "        return tokens + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # Transpose for MultiheadAttention (seq_len, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Generate causal mask (triangular lower matrix to prevent attending to future tokens)\n",
    "        seq_len = x.size(0)  # Sequence length after transpose\n",
    "        batch_size = x.size(1)  # Batch size after transpose\n",
    "        \n",
    "        # Adjust causal mask for batch size and number of heads\n",
    "        attn_mask = torch.tril(torch.ones((seq_len, seq_len), device=x.device)).bool()  # Convert to boolean\n",
    "        attn_mask = attn_mask.unsqueeze(0).expand(batch_size * n_head, -1, -1)\n",
    "\n",
    "        # Pass the mask to the attention layer\n",
    "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        x = x + attn_output\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Transpose back to (batch_size, seq_len, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + mlp_output\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyJoey(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyJoey, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings()\n",
    "        \n",
    "        # Decoder Blocks (based on n_layer_decoder)\n",
    "        self.decoder_blocks = nn.ModuleList([TransformerBlock() for _ in range(n_layer_decoder)])\n",
    "\n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # Get embeddings\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Apply decoder blocks with attention mask\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, key_padding_mask=key_padding_mask)  # Pass key_padding_mask to the block\n",
    "\n",
    "        # Layer norm and output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BabyJoey AutoUnit definition\n",
    "class BabyJoeyUnit(AutoUnit):\n",
    "    def __init__(self, module, device=None):\n",
    "        super().__init__(module=module, device=device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def compute_loss(self, state, data):\n",
    "        input_ids, attention_mask = data['input_ids'], data['attention_mask']\n",
    "        \n",
    "        # Ensure the attention mask is of type bool (for key_padding_mask)\n",
    "        key_padding_mask = (attention_mask == 0).bool()\n",
    "\n",
    "        # The model will now handle both causal masking (inside the TransformerBlock) and key_padding_mask\n",
    "        logits = self.module(input_ids, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        # Shift the input ids by one to get the target sequence\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return loss, logits\n",
    "\n",
    "    def configure_optimizers_and_lr_scheduler(self, module):\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(module.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "        \n",
    "        # For simplicity, we'll return `None` for the learning rate scheduler.\n",
    "        return optimizer, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for training\n",
    "class TestingCallback(Callback):\n",
    "    def on_train_start(self, state, unit):\n",
    "        print(\"Training started!\")\n",
    "    def on_train_end(self, state, unit):\n",
    "        print(\"Training ended!\")\n",
    "    def on_train_epoch_start(self, state, unit):\n",
    "        print(f\"Training epoch {unit.train_progress.num_epochs_completed} started!\")\n",
    "    def on_train_epoch_end(self, state, unit):\n",
    "        print(f\"Training epoch {unit.train_progress.num_epochs_completed} ended!\")\n",
    "    def on_train_step_start(self, state, unit):\n",
    "        print(f\"Training batch {unit.train_progress.num_steps_completed} started!\")\n",
    "    def on_train_step_end(self, state, unit):\n",
    "        print(f\"Training batch {unit.train_progress.num_steps_completed} ended!\")\n",
    "    def on_eval_start(self, state, unit):\n",
    "        print(\"Evaluation started!\")\n",
    "    def on_eval_end(self, state, unit):\n",
    "        print(\"Evaluation ended!\")\n",
    "    def on_eval_epoch_start(self, state, unit):\n",
    "        print(f\"Evaluation epoch {unit.eval_progress.num_epochs_completed} started!\")\n",
    "    def on_eval_epoch_end(self, state, unit):\n",
    "        print(f\"Evaluation epoch {unit.eval_progress.num_epochs_completed} ended!\")\n",
    "    def on_eval_step_start(self, state, unit):\n",
    "        print(f\"Evaluation batch {unit.eval_progress.num_steps_completed} started!\")\n",
    "    def on_eval_step_end(self, state, unit):\n",
    "        print(f\"Evaluation batch {unit.eval_progress.num_steps_completed} ended!\")\n",
    "    def on_exception(self, state, unit, exc: BaseException):\n",
    "        print(f\"Exception occurred: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device and initialize the BabyJoey model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BabyJoey().to(device)\n",
    "baby_joey_unit = BabyJoeyUnit(module=model, device=device)\n",
    "\n",
    "# Train the model using the defined AutoUnit and callback\n",
    "fit(\n",
    "    baby_joey_unit,\n",
    "    train_dataloader=training_dataloader,\n",
    "    eval_dataloader=validation_dataloader,\n",
    "    max_epochs=2,\n",
    "    callbacks=[TestingCallback()]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
